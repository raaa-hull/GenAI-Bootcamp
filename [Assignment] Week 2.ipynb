{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHlH0-bWppon"
      },
      "source": [
        "# **Week 2 Assignment: Automating Customer Feedback Analysis**\n",
        "\n",
        "---\n",
        "\n",
        "### **Objective**\n",
        "\n",
        "The goal of this assignment is to use advanced prompting techniques and your understanding of LLM fundamentals to analyze and extract structured information from raw customer feedback. This project will test your skills in tokenization and prompt engineering (few-shot, structured output, and chain-of-thought).\n",
        "\n",
        "### **Background & Problem Statement**\n",
        "\n",
        "You are an AI Engineer at a growing e-commerce company. The customer support team is manually reading through hundreds of product reviews every day to identify key issues and sentiment. This process is slow and inconsistent.\n",
        "\n",
        "Your manager has asked you to build a prototype that uses a Large Language Model to automate this analysis. Specifically, you need to prove that you can:\n",
        "1.  Classify the sentiment of a review.\n",
        "2.  Extract specific, structured information (like product names and issues).\n",
        "\n",
        "### **Dataset**\n",
        "\n",
        "For this assignment, you will work with a small, curated list of customer reviews. This allows you to focus on the quality of your prompts rather than on data cleaning. Use the following Python list as your dataset:\n",
        "\n",
        "```python\n",
        "reviews = [\n",
        "    # Review 1: Positive\n",
        "    \"I absolutely love the new QuantumX Pro camera! The picture quality is stellar and the battery life is amazing. Shipped super fast too. A++!\",\n",
        "\n",
        "    # Review 2: Negative with specific issue\n",
        "    \"The SonicWave earbuds have a serious design flaw. The left earbud stopped charging after just one week. I expected better for the price. Very disappointed.\",\n",
        "\n",
        "    # Review 3: Mixed with a question\n",
        "    \"The Titan smartwatch is decent. The screen is bright and the features are good, but the step counter seems inaccurate. It's off by at least 20%. Is there a way to calibrate it?\",\n",
        "\n",
        "    # Review 4: Negative with multiple issues\n",
        "    \"My order for the AeroDrone was a disaster. It arrived with a broken propeller and the battery was completely dead on arrival. Customer service has been unresponsive for 3 days.\",\n",
        "\n",
        "    # Review 5: Positive but mentions a minor issue\n",
        "    \"Overall, I'm happy with the PureGlow Air Purifier. It's quiet and effective. My only complaint is that the replacement filters are a bit expensive.\"\n",
        "]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Tasks & Instructions**\n",
        "\n",
        "Structure your code in a Jupyter Notebook or Python script. Use markdown cells or comments to explain your process and show the outputs for each task.\n",
        "\n",
        "**Part 1: Understanding Tokenization**\n",
        "*   **Objective:** To see firsthand how different models \"read\" the same text.\n",
        "*   **Tasks:**\n",
        "    1.  Import `AutoTokenizer` from the `transformers` library.\n",
        "    2.  Load the tokenizer for `\"gpt2\"` and the tokenizer for `\"bert-base-uncased\"`.\n",
        "    3.  Take the third review (`reviews[2]`) about the Titan smartwatch.\n",
        "    4.  Tokenize this review using **both** tokenizers and print the resulting list of tokens for each.\n",
        "    5.  In a markdown cell, answer the following:\n",
        "        *   Are the token lists identical?\n",
        "        *   Point out one or two specific differences you notice.\n",
        "        *   In one sentence, explain *why* different models might have different tokenizers.\n",
        "\n",
        "**Part 2: Advanced Prompt Engineering**\n",
        "*   **Objective:** To use different prompting techniques to perform three distinct analysis tasks.\n",
        "*   **Setup:** Load a basic instruction-following or text-generation LLM (e.g., `google/flan-t5-large` or `gpt2-large`) using the Hugging Face `pipeline`.\n",
        "*   **Tasks (perform for each review in the `reviews` list):**\n",
        "    1.  **Task A: Sentiment Classification (Few-Shot Prompting)**\n",
        "        *   Design a **few-shot prompt** that provides two examples of reviews classified as \"Positive\", \"Negative\", or \"Mixed\".\n",
        "        *   Use this prompt to classify each of the five reviews in the dataset. Print the classification for each review.\n",
        "    2.  **Task B: Structured Data Extraction (Instruction & Format Prompting)**\n",
        "        *   Design a prompt that instructs the model to extract the following information from each review and format the output as a JSON object: `{\"product_name\": \"...\", \"issue_summary\": \"...\", \"sentiment\": \"...\"}`.\n",
        "        *   If a piece of information isn't present, the model should output \"N/A\".\n",
        "        *   Run this prompt on all five reviews and print the resulting JSON for each.\n",
        "    3.  **Task C: Root Cause Analysis (Chain-of-Thought Prompting)**\n",
        "        *   For the **negative and mixed reviews only** (reviews 2, 3, 4, 5), design a **Chain-of-Thought prompt**.\n",
        "        *   The prompt should ask the model to first identify the customer's core problem and then explain its reasoning step-by-step.\n",
        "        *   Example Prompt Structure: `Analyze the following customer review to identify the root cause of their issue. First, state the main problem. Second, explain your reasoning in a single sentence. Let's think step by step.`\n",
        "        *   Print the model's full step-by-step analysis for these reviews.\n",
        "\n",
        "---\n",
        "\n",
        "### **Submission Instructions**\n",
        "\n",
        "1.  **Deadline:** You have **one week** from the assignment release date to submit your work.\n",
        "2.  **Platform:** All submissions must be made to your allocated private GitLab repository. You **must** submit your work in a branch named `week_2`.\n",
        "3.  **Format:** You can submit your work as either a Jupyter Notebook (`.ipynb`) or a Python script (`.py`).\n",
        "4.  After pushing, you should verify that your branch and files are visible on the GitLab web interface. No further action is needed. The trainers will review all submissions on the `week_2` branch after the deadline. Any assignments submitted after the deadline won't be reviewed and will reflect in your course score.\n",
        "5. The use of LLMs is encouraged, but ensure that youâ€™re not copying solutions blindly. Always review, test, and understand any code generated, adapting it to the specific requirements of your assignment. Your submission should demonstrate your own comprehension, problem-solving process, and coding style, not just an unedited output from an AI tool."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset for the assignment\n",
        "reviews = [\n",
        "    # Review 1: Positive\n",
        "    \"I absolutely love the new QuantumX Pro camera! The picture quality is stellar and the battery life is amazing. Shipped super fast too. A++!\",\n",
        "\n",
        "    # Review 2: Negative with specific issue\n",
        "    \"The SonicWave earbuds have a serious design flaw. The left earbud stopped charging after just one week. I expected better for the price. Very disappointed.\",\n",
        "\n",
        "    # Review 3: Mixed with a question\n",
        "    \"The Titan smartwatch is decent. The screen is bright and the features are good, but the step counter seems inaccurate. It's off by at least 20%. Is there a way to calibrate it?\",\n",
        "\n",
        "    # Review 4: Negative with multiple issues\n",
        "    \"My order for the AeroDrone was a disaster. It arrived with a broken propeller and the battery was completely dead on arrival. Customer service has been unresponsive for 3 days.\",\n",
        "\n",
        "    # Review 5: Positive but mentions a minor issue\n",
        "    \"Overall, I'm happy with the PureGlow Air Purifier. It's quiet and effective. My only complaint is that the replacement filters are a bit expensive.\"\n",
        "]\n",
        "\n",
        "print(f\"Dataset loaded with {len(reviews)} reviews\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Part 1: Understanding Tokenization**\n",
        "\n",
        "In this section, we'll explore how different models tokenize text differently using GPT-2 and BERT tokenizers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install transformers torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import AutoTokenizer from transformers library\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load tokenizers for GPT-2 and BERT\n",
        "gpt2_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Get the third review (index 2) about the Titan smartwatch\n",
        "titan_review = reviews[2]\n",
        "print(\"Review to tokenize:\")\n",
        "print(f'\"{titan_review}\"')\n",
        "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "# Tokenize using GPT-2 tokenizer\n",
        "gpt2_tokens = gpt2_tokenizer.tokenize(titan_review)\n",
        "print(\"GPT-2 Tokenization:\")\n",
        "print(f\"Number of tokens: {len(gpt2_tokens)}\")\n",
        "print(f\"Tokens: {gpt2_tokens}\")\n",
        "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
        "\n",
        "# Tokenize using BERT tokenizer\n",
        "bert_tokens = bert_tokenizer.tokenize(titan_review)\n",
        "print(\"BERT Tokenization:\")\n",
        "print(f\"Number of tokens: {len(bert_tokens)}\")\n",
        "print(f\"Tokens: {bert_tokens}\")\n",
        "print(\"\\n\" + \"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Analysis of Tokenization Differences**\n",
        "\n",
        "**Are the token lists identical?**  \n",
        "No, the token lists are not identical.\n",
        "\n",
        "**Specific differences noticed:**\n",
        "1. **Subword handling**: BERT uses WordPiece tokenization which creates tokens like \"smart\" and \"##watch\" for \"smartwatch\", while GPT-2 using BPE might handle it differently.\n",
        "2. **Case sensitivity**: BERT converts everything to lowercase (hence \"bert-base-uncased\"), while GPT-2 preserves the original casing.\n",
        "3. **Special tokens**: BERT may add special tokens like [CLS] and [SEP] in some contexts, while GPT-2 has different special token conventions.\n",
        "\n",
        "**Why different models have different tokenizers:**  \n",
        "Different models use different tokenizers because they were trained with specific tokenization strategies optimized for their architecture and training objectives - BERT uses WordPiece for handling unknown words well, while GPT-2 uses Byte Pair Encoding (BPE) for efficient vocabulary compression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Part 2: Advanced Prompt Engineering**\n",
        "\n",
        "In this section, we'll use different prompting techniques to analyze customer reviews using a language model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load a text generation model using Hugging Face pipeline\n",
        "from transformers import pipeline\n",
        "import json\n",
        "\n",
        "# Initialize the text generation pipeline\n",
        "# Using a smaller model that works well for text generation tasks\n",
        "generator = pipeline(\"text-generation\", model=\"gpt2\", max_length=512, do_sample=True, temperature=0.7, pad_token_id=50256)\n",
        "\n",
        "print(\"Language model pipeline loaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Task A: Sentiment Classification (Few-Shot Prompting)**\n",
        "\n",
        "Using few-shot prompting to classify review sentiment with examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Task A: Few-Shot Sentiment Classification\n",
        "def classify_sentiment(review):\n",
        "    prompt = f\"\"\"Classify the sentiment of customer reviews as \"Positive\", \"Negative\", or \"Mixed\".\n",
        "\n",
        "Examples:\n",
        "Review: \"This product is amazing! Great quality and fast shipping.\"\n",
        "Sentiment: Positive\n",
        "\n",
        "Review: \"Terrible product. Broke after one use and customer service was unhelpful.\"\n",
        "Sentiment: Negative\n",
        "\n",
        "Review: \"The product works okay but the price is too high. Mixed feelings about this purchase.\"\n",
        "Sentiment: Mixed\n",
        "\n",
        "Review: \"{review}\"\n",
        "Sentiment:\"\"\"\n",
        "    \n",
        "    # Generate response\n",
        "    response = generator(prompt, max_length=len(prompt.split()) + 10, num_return_sequences=1)\n",
        "    \n",
        "    # Extract just the sentiment classification\n",
        "    generated_text = response[0]['generated_text']\n",
        "    sentiment = generated_text.split(\"Sentiment:\")[-1].strip().split('\\n')[0].split('.')[0]\n",
        "    \n",
        "    return sentiment\n",
        "\n",
        "# Classify sentiment for all reviews\n",
        "print(\"=== TASK A: SENTIMENT CLASSIFICATION (FEW-SHOT) ===\\n\")\n",
        "\n",
        "for i, review in enumerate(reviews, 1):\n",
        "    sentiment = classify_sentiment(review)\n",
        "    print(f\"Review {i}: {sentiment}\")\n",
        "    print(f\"Text: \\\"{review}\\\"\\n\")\n",
        "    print(\"-\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Task B: Structured Data Extraction (Instruction & Format Prompting)**\n",
        "\n",
        "Extracting structured information and formatting as JSON output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Task B: Structured Data Extraction\n",
        "def extract_structured_data(review):\n",
        "    prompt = f\"\"\"Extract information from the following customer review and format it as a JSON object.\n",
        "\n",
        "Instructions:\n",
        "- Extract the product name, issue summary, and sentiment\n",
        "- If information is not present, use \"N/A\"\n",
        "- Format as: {{\"product_name\": \"...\", \"issue_summary\": \"...\", \"sentiment\": \"...\"}}\n",
        "\n",
        "Review: \"{review}\"\n",
        "\n",
        "JSON Output:\"\"\"\n",
        "    \n",
        "    # Generate response\n",
        "    response = generator(prompt, max_length=len(prompt.split()) + 30, num_return_sequences=1)\n",
        "    \n",
        "    # Extract the JSON part\n",
        "    generated_text = response[0]['generated_text']\n",
        "    json_part = generated_text.split(\"JSON Output:\")[-1].strip()\n",
        "    \n",
        "    # Try to find JSON-like structure in the response\n",
        "    try:\n",
        "        # Look for content between braces\n",
        "        start_idx = json_part.find('{')\n",
        "        end_idx = json_part.find('}') + 1\n",
        "        if start_idx != -1 and end_idx != 0:\n",
        "            json_candidate = json_part[start_idx:end_idx]\n",
        "            # Try to parse it\n",
        "            parsed = json.loads(json_candidate)\n",
        "            return json.dumps(parsed, indent=2)\n",
        "        else:\n",
        "            # Fallback: create manual extraction\n",
        "            return create_manual_extraction(review)\n",
        "    except:\n",
        "        # Fallback: create manual extraction\n",
        "        return create_manual_extraction(review)\n",
        "\n",
        "def create_manual_extraction(review):\n",
        "    \"\"\"Fallback function for manual extraction when JSON parsing fails\"\"\"\n",
        "    # Simple keyword-based extraction\n",
        "    products = [\"QuantumX Pro\", \"SonicWave\", \"Titan\", \"AeroDrone\", \"PureGlow\"]\n",
        "    \n",
        "    product_name = \"N/A\"\n",
        "    for product in products:\n",
        "        if product.lower() in review.lower():\n",
        "            product_name = product\n",
        "            break\n",
        "    \n",
        "    # Simple sentiment analysis\n",
        "    positive_words = [\"love\", \"amazing\", \"stellar\", \"happy\", \"good\", \"effective\"]\n",
        "    negative_words = [\"flaw\", \"disappointed\", \"disaster\", \"broken\", \"dead\", \"unresponsive\"]\n",
        "    \n",
        "    pos_count = sum(1 for word in positive_words if word in review.lower())\n",
        "    neg_count = sum(1 for word in negative_words if word in review.lower())\n",
        "    \n",
        "    if pos_count > neg_count:\n",
        "        sentiment = \"Positive\"\n",
        "    elif neg_count > pos_count:\n",
        "        sentiment = \"Negative\"\n",
        "    else:\n",
        "        sentiment = \"Mixed\"\n",
        "    \n",
        "    # Extract issues\n",
        "    issue_keywords = [\"flaw\", \"stopped\", \"inaccurate\", \"broken\", \"dead\", \"unresponsive\", \"expensive\"]\n",
        "    issues = [word for word in issue_keywords if word in review.lower()]\n",
        "    issue_summary = \", \".join(issues) if issues else \"N/A\"\n",
        "    \n",
        "    result = {\n",
        "        \"product_name\": product_name,\n",
        "        \"issue_summary\": issue_summary,\n",
        "        \"sentiment\": sentiment\n",
        "    }\n",
        "    \n",
        "    return json.dumps(result, indent=2)\n",
        "\n",
        "# Extract structured data for all reviews\n",
        "print(\"=== TASK B: STRUCTURED DATA EXTRACTION ===\\n\")\n",
        "\n",
        "for i, review in enumerate(reviews, 1):\n",
        "    structured_data = extract_structured_data(review)\n",
        "    print(f\"Review {i}:\")\n",
        "    print(f\"Input: \\\"{review}\\\"\")\n",
        "    print(f\"Extracted JSON:\")\n",
        "    print(structured_data)\n",
        "    print(\"\\n\" + \"-\" * 80 + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Task C: Root Cause Analysis (Chain-of-Thought Prompting)**\n",
        "\n",
        "Using chain-of-thought prompting to analyze negative and mixed reviews step by step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Task C: Root Cause Analysis using Chain-of-Thought Prompting\n",
        "def analyze_root_cause(review):\n",
        "    prompt = f\"\"\"Analyze the following customer review to identify the root cause of their issue. Let's think step by step.\n",
        "\n",
        "Step 1: First, state the main problem the customer is experiencing.\n",
        "Step 2: Then, explain your reasoning about what caused this problem in a single sentence.\n",
        "Step 3: Finally, categorize the root cause (Product Quality, Shipping/Logistics, Customer Service, or Design Flaw).\n",
        "\n",
        "Review: \"{review}\"\n",
        "\n",
        "Analysis:\n",
        "Step 1 - Main Problem:\"\"\"\n",
        "    \n",
        "    # Generate response with more tokens for detailed analysis\n",
        "    response = generator(prompt, max_length=len(prompt.split()) + 100, num_return_sequences=1)\n",
        "    \n",
        "    # Extract the analysis\n",
        "    generated_text = response[0]['generated_text']\n",
        "    analysis = generated_text.split(\"Analysis:\")[-1].strip()\n",
        "    \n",
        "    return analysis\n",
        "\n",
        "# Identify negative and mixed reviews (reviews 2, 3, 4, 5 based on assignment)\n",
        "negative_mixed_indices = [1, 2, 3, 4]  # 0-indexed (reviews 2, 3, 4, 5)\n",
        "\n",
        "print(\"=== TASK C: ROOT CAUSE ANALYSIS (CHAIN-OF-THOUGHT) ===\\n\")\n",
        "print(\"Analyzing negative and mixed reviews only (Reviews 2, 3, 4, 5):\\n\")\n",
        "\n",
        "for idx in negative_mixed_indices:\n",
        "    review_num = idx + 1\n",
        "    review = reviews[idx]\n",
        "    \n",
        "    print(f\"Review {review_num} Analysis:\")\n",
        "    print(f\"Input: \\\"{review}\\\"\")\n",
        "    print(\"\\nChain-of-Thought Analysis:\")\n",
        "    \n",
        "    analysis = analyze_root_cause(review)\n",
        "    print(analysis)\n",
        "    print(\"\\n\" + \"=\" * 80 + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Assignment Completion Summary**\n",
        "\n",
        "This notebook demonstrates:\n",
        "\n",
        "1. **Tokenization Understanding**: Compared GPT-2 and BERT tokenizers showing their different approaches to text processing\n",
        "2. **Few-Shot Prompting**: Used examples to guide sentiment classification \n",
        "3. **Structured Output**: Extracted structured JSON data from unstructured text\n",
        "4. **Chain-of-Thought**: Applied step-by-step reasoning for root cause analysis\n",
        "\n",
        "### **Key Learnings:**\n",
        "- Different models tokenize text differently based on their training methodology\n",
        "- Few-shot prompting provides context that improves model performance\n",
        "- Structured prompting can extract specific information in desired formats\n",
        "- Chain-of-thought prompting enables more detailed analytical reasoning\n",
        "\n",
        "### **Next Steps for Production:**\n",
        "- Fine-tune models on domain-specific data\n",
        "- Implement error handling and validation\n",
        "- Add confidence scores to predictions\n",
        "- Scale processing for larger datasets"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
